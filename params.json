{"name":"Hadoop","tagline":"","body":"This first of a four-part series on how to chain and manage multiple MapReduce Jobs.\r\n================================\r\nChaining and Managing Multiple MapReduce Jobs Part 1: Using a BASH Shell\r\n\r\n\r\nWhile a single MapReduce job may be sufficient for simple tasks it is fairly common for 2, 3 or more jobs to be chained together to accomplish a specific need. This series will show you how to chain jobs using the Shell, within the driver code, Job Control and lastly Oozie.\r\n\r\n\r\nMultiple MapReduce Jobs\r\n-------------------------\r\n  \r\nThis takes a basic WordCount problem and breaks it up into two MapReduce jobs. The first simply counts and  outputs  a Key (Count) and Value (Word). The second \r\n\r\nMapReduce job will be a Map only job. The sole purpose of this second job is to swap the Key and Value and output in sorted order. The Shell script will manage the execution of both jobs. The final output will be Key (Word) and Value (Count).\r\n\r\n\r\nA Shell script is simple to implement. Its fast and does not require any compiling if more jobs are added. The single script can be split into two sections.   \r\n\r\nPart 1) Contains the variables to define the Jar Files, Input Directory (shakespeare) as well as the commands to execute the MapReduce Jobs.\r\n\t**Note: ** COMMON_TEMP serves as the output of the 1st job and input of the second job.\r\n\r\n    #!/bin/bash\r\n    # Jar file definition\r\n    MAPREDUCE_JAR_JOB1=\"WordCount.jar\"\r\n    MAPREDUCE_JAR_JOB2=\"WordCountTranspose.jar\"\r\n\r\n    MAIN_CLASS_JOB1=\"WordCount\"\r\n    MAIN_CLASS_JOB2=\"WordCountTranspose\"\r\n\r\n    HADOOP=\"$( which hadoop )\"\r\n\r\n    INPUT_DIRECTORY=\"shakespeare\"\r\n    COMMON_TEMP=\"TEMP_DIR\"\r\n    FINAL_OUTPUT=\"JOB_CHAIN_RESULTS\"\r\n\r\n\r\n    JOB_1_CMD=\"${HADOOP} jar ${MAPREDUCE_JAR_JOB1} ${MAIN_CLASS_JOB1}  \\ \r\n    ${INPUT_DIRECTORY} ${COMMON_TEMP}\"\r\n    \r\n    JOB_2_CMD=\"${HADOOP} jar ${MAPREDUCE_JAR_JOB2} ${MAIN_CLASS_JOB2} \\ \r\n    ${COMMON_TEMP} ${FINAL_OUTPUT}\"\r\n\r\n \r\n    CAT_FINAL_OUTPUT_CMD=\"${HADOOP} fs -cat ${FINAL_OUTPUT}/part-*\"\r\n    CLEANUP_CMD=\"${HADOOP} fs -rm -r ${COMMON_TEMP} ${FINAL_OUTPUT}\"\r\n    LOG_FILE=\"LOG_`date +%s`.txt\"\r\n\r\n\r\nPart 2) Manages the execution of the commands. This will ensure that the jobs run in sequential order and will exit if any jobs fail.\r\n\r\n    {\r\n    ${CLEANUP_CMD}\r\n    ${JOB_1_CMD}\r\n    if [ $? -ne 0 ]\r\n    then\r\n    echo \"ERROR FIRST JOB. SEE LOG.\"\r\n    ${CLEANUP_CMD} \r\n    exit $?\r\n    fi\r\n    echo ${JOB_2_CMD}\r\n    ${JOB_2_CMD}\r\n    if [ $? -ne 0 ]\r\n    then\r\n    echo \"ERROR SECOND JOB. SEE LOG.\"\r\n    ${CLEANUP_CMD}\r\n    exit $?\r\n    fi\r\n    echo ${CAT_FINAL_OUTPUT_CMD}\r\n    ${CAT_FINAL_OUTPUT_CMD}\r\n    ${CLEANUP_CMD}\r\n    exit 0\r\n    } &> ${LOG_FILE}\r\n\r\n\r\nPart 3) Lastly the LOG_FILE will show the output of the two MapReduce jobs and any errors if they exist.\r\n\r\n\r\nSummary\r\n-------------------------\r\n\r\nAs noted in above example you can scale this up to many jobs. The advantages of this method is that since it is done in the shell no recompiling is needed. One con is that jobs run in this manner can only be done sequentially. The following parts in this series will show you more efficient ways to managing large number of jobs. \r\n\r\n\r\nReferences\r\n--------------------\r\n\r\nMiner and Shook. _MapReduce Design Patterns: Building Effective Algorithms and Analytics for Hadoop and Other Systems._ O'Reilly 2013","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}