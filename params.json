{"name":"Hadoop","tagline":"Chaining and Managing Multiple Hadoop MapReduce Jobs Part 1: Using a BASH Shell","body":"# Chaining and Managing Multiple Hadoop MapReduce Jobs Part 1: Using a BASH Shell\r\n\r\nMichael Prom & Brad Rubin\r\n11/8/2013\r\n\r\n### This first of a four-part series showing how to chain and manage multiple MapReduce Jobs.\r\n---\r\nWhile a single MapReduce job may be sufficient for certain tasks, there may be instances where 2 or more jobs will be required.  Instead of executing jobs manually, we can automate this process using various methods.  \r\n\r\nThis series will show you how to chain multiple jobs together using four techniques: a shell script, within the driver code itself, using the JobControl class, and the Oozie: workflow scheduler.\r\n\r\n\r\n## The Two MapReduce Jobs\r\n  \r\nThis is a basic WordCount problem which uses two MapReduce jobs. The first, is a standard WordCount problem. It simply outputs the word as the key and the count of the word as the value.  The second MapReduce job inverts the key and the value in the mapper. The output is a sorted list of word counts as the key and the associated word as a value.\r\n\r\nThis same concept can be applied to other situations as well. There may be cases where you want to use a MapReduce job to pre-process or sort the data before feeding it into another job. \r\n\r\n## The Shell Script\r\n\r\nUsing a shell script to manage the sequential execution of the jobs. \r\n\r\nWe will use the bash shell since it the default in most Linux distributions. Other shells will work as well. The script contains pre-define commands to be executed. **Note:**  Managing jobs using the shell will only allow jobs to be run sequentially. If your specific need requires the jobs to be run in parallel see the future how-tos in this series.   \r\n\r\nThe first step requires you to create an executable script file. We called this \"WordCountInvert.sh\". (Ensure that the permissions are set correcting using \"CHMOD\"). The bash script contains the variables to define the JAR files, input directory (shakespeare), and the commands to verify the result of the MapReduce Jobs. This script will output the results into a log file.  \r\n\r\nThe script file should look like the one below. Ensure that the variables are set for your needs.\r\n\r\n**Note:** COMMON_TEMP serves as the output filename of the 1st job, which is also the input to the second job.   \r\nThe LOG_FILE will show the output of the two MapReduce jobs and any errors if they exist.\r\n\r\n    #!/bin/bash\r\n    # This script will run two MapReduce jobs. \r\n    # Jar file definition\r\n\r\n    MAPREDUCE_JAR_JOB1=\"WordCount.jar\"\r\n    MAPREDUCE_JAR_JOB2=\"WordCountInvert.jar\"\r\n    \r\n    # Job class definition. There are two main classes in this job. WordCount and WordCountInvert\r\n    MAIN_CLASS_JOB1=\"WordCount\"\r\n    MAIN_CLASS_JOB2=\"WordCountInvert\"\r\n    HADOOP=\"$( which hadoop )\"\r\n    # Definition of the input directory. The shakespeare directory contains multiple text files. \r\n    INPUT_DIRECTORY=\"shakespeare\"\r\n    COMMON_TEMP=\"TEMP_DIR\"\r\n    # This is the final job output directory.\r\n    FINAL_OUTPUT=\"JOB_CHAIN_RESULTS\"\r\n    #Define the two commands to execute the jobs. \r\n    JOB_1_CMD=\"${HADOOP} jar ${MAPREDUCE_JAR_JOB1} ${MAIN_CLASS_JOB1}  \\ \r\n    ${INPUT_DIRECTORY} ${COMMON_TEMP}\"\r\n    \r\n     \r\n    JOB_2_CMD=\"${HADOOP} jar ${MAPREDUCE_JAR_JOB2} ${MAIN_CLASS_JOB2} \\\r\n    ${COMMON_TEMP} ${FINAL_OUTPUT}\"\r\n    # Define the command to cat the output of the directory.\r\n    CAT_FINAL_OUTPUT_CMD=\"${HADOOP} fs -cat  \r\n    ${FINAL_OUTPUT}/part-*\"\r\n    CLEANUP_CMD=\"${HADOOP} fs -rm -r ${COMMON_TEMP} ${FINAL_OUTPUT}\"\r\n    LOG_FILE=\"LOG_`date +%s`.txt\"\r\n    \r\n    #This command will execute each job and determine status. \r\n    #Results will be displayed in the log file. \r\n    {\r\n    # Cleanup Command\r\n    ${CLEANUP_CMD}\r\n     \r\n    ${JOB_1_CMD}\r\n    if [ $? -ne 0 ]\r\n    then\r\n    echo \"ERROR FIRST JOB. SEE LOG.\"\r\n    ${CLEANUP_CMD}\r\n    exit $?\r\n    fi\r\n    echo ${JOB_2_CMD}\r\n    ${JOB_2_CMD}\r\n    if [  \r\n    $? -ne 0 ]\r\n    then\r\n    echo \"ERROR SECOND JOB. SEE LOG.\"\r\n    ${CLEANUP_CMD}\r\n    exit $?\r\n    fi\r\n    echo ${CAT_FINAL_OUTPUT_CMD}\r\n    ${CAT_FINAL_OUTPUT_CMD}\r\n     \r\n    ${CLEANUP_CMD}\r\n    exit 0\r\n    } &> ${LOG_FILE}\r\n\r\n## File Execution\r\n\r\nYou can simply execute the file using ./WordCountInvert.csh from the command line (ensure x is set in the permissions). You can place multiple echo commands within the file and view the log file when complete.\r\n\r\n\r\n## Pros and Cons\r\n\r\nAs noted in above, you can scale this up to many jobs. Since it is done in the shell, no recompiling is needed. However, as noted jobs run in this manner can only be run sequentially. As the number of jobs increases the more lines needed in the script file. The following parts in this series will describe more efficient ways to managing large number of jobs. \r\n\r\n\r\n## References\r\n\r\nMiner and Shook. _MapReduce Design Patterns: Building Effective Algorithms and Analytics for Hadoop and Other Systems._ O'Reilly 2013\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}